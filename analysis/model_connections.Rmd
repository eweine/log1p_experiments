---
title: "Connections Between Poisson Matrix Factorization Methods"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-10-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Three Different Models

Suppose that for $(i, j) \in \{1, \dots, n\} \times \{1, \dots, m\}$, $$y_{ij} \sim \textrm{Poisson}(\lambda_{ij}).$$ The log-likelihood of this model is $$\sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log(\lambda_{ij}) - \lambda_{ij}.$$

### Topic Model / Poisson NMF with Identity Link

Suppose that

\begin{align}
y_{ij} &\sim \textrm{Poisson}(\lambda_{ij}) \\
\boldsymbol{\Lambda} &= \boldsymbol{L}\boldsymbol{F}^{T},
\end{align}

where $\boldsymbol{L}$ and $\boldsymbol{F}$ are both rank-K elementwise non-negative matrices. Define the log-likelihood of this model as 

$$\ell_{TM}(\boldsymbol{L}, \boldsymbol{F}) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(\sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) - \sum_{k = 1}^{K} \ell_{ik} f_{jk}.$$

### GLM-PCA

Suppose that

\begin{align}
y_{ij} &\sim \textrm{Poisson}(\lambda_{ij}) \\
\log(\lambda_{ij}) &= b_{ij} \\
\boldsymbol{B} &= \boldsymbol{L}\boldsymbol{F}^{T},
\end{align}

where $\boldsymbol{L}$ and $\boldsymbol{F}$ are both rank-K matrices. Define the log-likelihood of this model as 

$$\ell_{GPCA}(\boldsymbol{L}, \boldsymbol{F}) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \left(\sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) - \exp\left(\sum_{k = 1}^{K} \ell_{ik} f_{jk}\right).$$

### Poisson NMF with log1p Link

Suppose that

\begin{align}
y_{ij} &\sim \textrm{Poisson}(\lambda_{ij}) \\
\log\left(1 + \frac{\lambda_{ij}}c\right) &= b_{ij} \\
\boldsymbol{B} &= \boldsymbol{L}\boldsymbol{F}^{T},
\end{align}

where $\boldsymbol{L}$ and $\boldsymbol{F}$ are both rank-K elementwise non-negative matrices and $c$ is a positive constant. Define the log-likelihood of this model as 

$$\ell_{log1p}(\boldsymbol{L}, \boldsymbol{F}, c) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(c \cdot \exp\left\{ \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - c \right) - c \cdot \exp\left( \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) + c.$$

## Unifying the Three Models

Suppose that we fix each value of $\lambda_{ij}$ in the log1p model. Then, if we set $c$ to be very large, then $\frac{\lambda_{ij}}{c}$ will be small and thus $\log\left(1 + \frac{\lambda_{ij}}{c}\right) \approx \frac{\lambda_{ij}}{c}$. Thus, when $c$ is large we should expect that the log1p model should look like the topic model. Now, suppose that we set $c$ to be a very small (positive) number. Then, $\frac{\lambda_{ij}}{c}$ will be large and thus $\log\left(1 + \frac{\lambda_{ij}}{c}\right) \approx \log\left(\frac{\lambda_{ij}}{c}\right).$ In this case, we should expect that the log1p model will act like GLM-PCA. 

# Connection of log1p Model to the Topic Model

Formally, we have 

$$\lim_{c \rightarrow \infty} \ell_{log1p}\left(\frac{1}{\sqrt{c}}\boldsymbol{L}, \frac{1}{\sqrt{c}}\boldsymbol{F}, c\right) = \ell_{TM}(\boldsymbol{L}, \boldsymbol{F})$$

We prove this below:

First, note that

\begin{equation*}
\ell_{log1p}\left(\frac{1}{\sqrt{c}}\boldsymbol{L}, \frac{1}{\sqrt{c}}\boldsymbol{F}, c\right) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(c \cdot \exp\left\{ \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - c \right) - c \cdot \exp\left( \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) + c
\end{equation*}

Now, separating out the term that depends on $c$ and taking the limit, we have

\begin{align*}
\lim_{c \rightarrow \infty} c \cdot \exp\left\{ \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - c &= \lim_{c \rightarrow \infty} c \cdot \left[ \exp\left\{ \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - 1 \right]
\end{align*}

Now, substitute in $b_{ij} = \sum_{k = 1}^{K} \ell_{ik} f_{jk}$ and let $x = \frac{b_{ij}}{c}$. Then, we can write

$$\lim_{c \rightarrow \infty} c \cdot \left[ \exp\left\{ \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - 1 \right] = \lim_{x \rightarrow 0^{+}} \frac{b_{ij}}{x} (e^{x} - 1)$$

Now, recall that the Taylor series expansion of $e^{x}$ about $x = 0$ is

$$e^{x} = \sum_{n = 0}^{\infty} \frac{x^{n}}{n!} = 1 + x + \frac{x^{2}}{2!} + \frac{x^{3}}{3!} + \dots$$
Thus, the Taylor series expansion of $e^{x} - 1$ about $x = 0$ is

$$e^{x} - 1 = \sum_{n = 1}^{\infty} \frac{x^{n}}{n!} = x + \frac{x^{2}}{2!} + \frac{x^{3}}{3!} + \dots$$

Now, we can write 

\begin{align*}
\lim_{x \rightarrow 0^{+}} \frac{b_{ij}}{x} (e^{x} - 1) &= \lim_{x \rightarrow 0^{+}} \frac{b_{ij}}{x} \left(x + \frac{x^{2}}{2!} + \frac{x^{3}}{3!} + \dots \right) \\
&= \lim_{x \rightarrow 0^{+}} \left(b_{ij} + \frac{b_{ij}x}{2!} + \frac{b_{ij} x^{2}}{3!} + \dots \right) \\
&= b_{ij}.
\end{align*}

Substituting this back into the equation above, we have

$$\lim_{c \rightarrow \infty} c \cdot \left[ \exp\left\{ \frac{1}{c} \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - 1 \right] = \sum_{k = 1}^{K} \ell_{ik} f_{jk}.$$

Finally, substituting this back into the equation for $\ell_{log1p}$, we have

$$\lim_{c \rightarrow \infty} \ell_{log1p}\left(\frac{1}{\sqrt{c}}\boldsymbol{L}, \frac{1}{\sqrt{c}}\boldsymbol{F}, c\right)  = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(\sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) - \sum_{k = 1}^{K} \ell_{ik} f_{jk} = \ell_{TM}(\boldsymbol{L}, \boldsymbol{F})$$

# Connection of log1p Model to GLM-PCA

To make the connection with GLM-PCA explicit, we have to slightly modify the log1p model. Specifically, we now take

\begin{align}
y_{ij} &\sim \textrm{Poisson}(\lambda_{ij}) \\
\log\left(1 + \frac{\lambda_{ij}}c\right) &= \alpha + b_{ij} \\
\boldsymbol{B} &= \boldsymbol{L}\boldsymbol{F}^{T},
\end{align}

where the variables are defined as above except for $\alpha$, which is a positive constant. We can write the log-likelihood of this model as 

$$\ell_{log1p_\alpha}(\boldsymbol{L}, \boldsymbol{F}, c, \alpha) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(c \cdot \exp\left\{ \alpha + \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - c \right) - c \cdot \exp\left( \alpha + \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) + c.$$
Now, formally we will prove that 

$$\lim_{c \rightarrow 0^{+}} \ell_{log1p_\alpha}\left(\boldsymbol{L}, \boldsymbol{F}, c, \log(1/c) \right) = \ell_{GPCA}(\boldsymbol{L}, \boldsymbol{F}).$$

First, we can write

$$\ell_{log1p_\alpha}(\boldsymbol{L}, \boldsymbol{F}, c, \log(1/c)) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \log\left(c \cdot \exp\left\{ \log(1/c) + \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right\} - c \right) - c \cdot \exp\left( \log(1/c) + \sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) + c.$$

Now, observe that 

\begin{align*}
\lim_{c \rightarrow 0^{+}} c [\exp\{\log(1 / c) + x\} - 1] &= \lim_{c \rightarrow 0^{+}} c [\exp\{\log(1 / c)\}\exp(x) - 1] \\
&= \lim_{c \rightarrow 0^{+}} c \frac{1}{c} \exp(x) - c \\
&= \exp(x).
\end{align*}

Plugging this relationship back into the equation for the log-likelihood, we have

$$\lim_{c \rightarrow 0^{+}} \ell_{log1p_\alpha}(\boldsymbol{L}, \boldsymbol{F}, c, \log(1/c)) = \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij} \left(\sum_{k = 1}^{K} \ell_{ik} f_{jk} \right) - \exp\left(\sum_{k = 1}^{K} \ell_{ik} f_{jk}\right) = \ell_{GPCA}(\boldsymbol{L}, \boldsymbol{F}).$$

# Experiments

Below, I investigate empirically if larger values of $c$ allow the `log1p` model to converge to either the topic model or GLMPCA.

First, I generate data from a rank-4 topic model and run the `log1p` model until convergence with increasing values of $c$. The dashed red line below represents the log-likelihood of the topic model obtained via `fastTopics`.

```{r, eval=FALSE}
set.seed(1)
n <- 500
p <- 250
K <- 4

library(distr)
library(Matrix)

l_dist <- UnivarMixingDistribution(
  Unif(0,0.05),
  Exp(rate = 1.75),
  mixCoeff = rep(1/2,2)
)

f_dist <- UnivarMixingDistribution(
  Unif(0,0.05),
  Exp(rate = 1.75),
  mixCoeff = rep(1/2,2)
)

l_sampler <- distr::r(l_dist)
f_sampler <- distr::r(f_dist)

LL <- matrix(
  data = l_sampler(n = n * K),
  nrow = n,
  ncol = K
)

FF <- matrix(
  data = f_sampler(n = p * K),
  nrow = p,
  ncol = K
)

Lambda <- LL %*% t(FF)

Y <- matrix(
  data = rpois(n = prod(dim(Lambda)), lambda = as.vector(Lambda)),
  nrow = n,
  ncol = p
)

Y_dense <- Y
Y <- as(Y, "CsparseMatrix")

c_vec <- c(
  0.01, 0.1, 0.25, 0.5, 0.75, 1, 2.5,
  5, 7.5, 10, 25, 50, 75, 100, 250,
  500, 750, 1000, 2500, 5000, 7500, 10000
  )
ll_vec <- numeric(length(c_vec))

for (i in 1:length(c_vec)) {
  
  set.seed(1)
  log1p <- passPCA::fit_factor_model_log1p_exact(
    Y = Y,
    K = 4,
    maxiter = 10000,
    s = rep(c_vec[i], n)
  )
  
  H <- exp(log1p$U %*% t(log1p$V)) - 1
  
  ll_vec[i] <- sum(
    dpois(
      x = as.vector(Y_dense),
      lambda = c_vec[i] * as.vector(H),
      log = TRUE
    )
  )
  
}

library(fastTopics)

set.seed(1)
tm <- fit_poisson_nmf(
  X = Y, k = 4,
  numiter = 1000
)

ll_tm <- sum(
  dpois(
    x = as.vector(Y_dense),
    lambda = as.vector(tm$L %*% t(tm$F)),
    log = TRUE
  )
)
```

```{r, echo=FALSE}
out_list <- readr::read_rds(
  "~/Documents/data/passPCA/experiment_results/c_scaling_nmf.rds"
  )

library(ggplot2)

ggplot(data = out_list$ll_df) +
  geom_point(aes(x = c, y = ll), size = 1.5) +
  scale_x_log10() +
  ylab("log-likelihood") +
  geom_hline(yintercept = out_list$tm_ll, color = "red", linetype = "dashed") +
  cowplot::theme_cowplot()
```


