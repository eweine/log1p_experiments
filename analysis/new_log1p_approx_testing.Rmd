---
title: "Objective evaluation of log1p flash normal approximation"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-08-14"
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

I wanted to try out Matthew's idea of a normal approximation to the log1p model in the context of matrix factorization. I generated data from the model:

\begin{align*}
y_{ij} &\sim \text{Poisson}(\lambda_{ij}) \\
\log(1 + \lambda_{ij}) &= H_{ij} \\
H &= LF^{T},
\end{align*}

where each element of $L$ and $F$ are generated from a mixture with $\frac{2}{3}$ mass at $0$ and $\frac{1}{3}$ mass on an $\text{Exponential}(3)$ distribution.

```{r}
library(flashier)
library(distr)

n <- 1500
p <- 750
K <- 5

l_dist <- UnivarMixingDistribution(
  Dirac(0),
  Exp(3),
  mixCoeff = c(2/3,1/3)
)

f_dist <- UnivarMixingDistribution(
  Dirac(0),
  Exp(3),
  mixCoeff = c(2/3,1/3)
)

l_sampler <- distr::r(l_dist)
f_sampler <- distr::r(f_dist)
```

In this case, I generated datasets of $1500 \times 750$ where the true rank of $H$ is $5$. After removing rows and columns containing all $0$s, the data are about $92\%$ sparse and have a maximum value around $300 - 500$. This seems fairly realistic for scRNA-seq data.

Then, I fit the following 3 models to the data over 10 separate simulations:

(1) MLE of log1p Poisson matrix factorization model with K set to 10. I used a quadratic approximation to terms in the log-likelihood corresponding to 0 counts, as discussed previously. I used K = 10 here because I thought it was a bit unfair to initialize the MLE at the true rank when I'm letting the other methods naturally determine the true number of factors. Alternatively, I could have just fixed K = 5 for all methods. However, I was interested in comparing how many factors each flash model ended up using.

(2) non-negative flash on the log1p transformed counts, with a constant value of S for all values. I used a greedy initialization and then a backfit.

(3) flash on the log1p transformed counts where all $0$ counts were set to $-1$ and S set according to Matthew's approximation. Using this model, I tried both (a) a greedy initialization followed by a backfit and (b) initializing with (1) followed by a backfit.

```{r, eval=FALSE}
get_n_factors_flash <- function(f) {

  n_factors <- f$n_factors

  for (j in 1:n_factors) {

    if (all(f$F_pm[, j] < 1e-12)) {

      n_factors <- n_factors - 1

    }

  }

  return(n_factors)

}

n_sims <- 10

mse_mle_vec <- c()
mse_log1p_old_vec <- c()
mse_log1p_new_mle_vec <- c()
mse_log1p_new_greedy_vec <- c()

factors_log1p_old_vec <- c()
factors_log1p_new_mle_vec <- c()
factors_log1p_new_greedy_vec <- c()

for (i in 1:n_sims) {

  set.seed(i)

  LL <- matrix(
    data = l_sampler(n * K),
    nrow = n,
    ncol = K
  )

  FF <- matrix(
    data = f_sampler(p * K),
    nrow = p,
    ncol = K
  )

  Lambda <- exp(tcrossprod(LL, FF)) - 1
  y <- rpois(n = length(as.vector(Lambda)), lambda = as.vector(Lambda))

  Y <- matrix(
    data = y,
    nrow = n,
    ncol = p
  )

  rownames(Y) <- paste0("cell", 1:n)
  colnames(Y) <- paste0("gene", 1:p)

  B <- tcrossprod(LL, FF)

  rownames(B) <- paste0("cell", 1:n)
  colnames(B) <- paste0("gene", 1:p)

  Y <- Y[rowSums(Y) > 0, ]
  Y <- Y[, colSums(Y) > 0]

  B <- B[rownames(B) %in% rownames(Y), ]
  B <- B[, colnames(B) %in% colnames(Y)]

  Y_trans <- log1p(Y)

  x  <- rpois(1e7, 1/n)
  s1 <- sd(log(x + 1))

  fit_flash_old <- flash(
    data = as(Y_trans, "sparseMatrix"),
    S = s1,
    ebnm_fn = ebnm::ebnm_point_exponential,
    backfit = TRUE
  )

  library(passPCA)

  log1p_fit <- fit_factor_model_log1p_quad_approx_sparse(
    Y = as(Y, "sparseMatrix"),
    K = 10,
    approx_range = c(0, 1.25),
    maxiter = 500
  )

  S <- as.matrix(Y / ((1 + Y) ^ 2))
  S <- apply(S, c(1, 2), function(x){if(x == 0){1}else{x}})

  Y_trans2 <- apply(Y_trans, c(1, 2), function(x){if(x == 0){-1}else{x}})

  fit_flash_new_mle <- flash_init(data = Y_trans2, S = sqrt(S))
  fit_flash_new_mle <- flash_factors_init(
    flash = fit_flash_new_mle,
    init = list(
      log1p_fit$U, log1p_fit$V
    ),
    ebnm_fn = ebnm::ebnm_point_exponential
  )

  fit_flash_new_mle <- flash_backfit(fit_flash_new_mle)
  fit_flash_new_greedy <- flash(data = Y_trans2, S = sqrt(S), backfit = TRUE)


  B_mle <- tcrossprod(log1p_fit$U, log1p_fit$V)
  B_log1p_old <- tcrossprod(fit_flash_old$L_pm, fit_flash_old$F_pm)
  B_log1p_new_mle <- tcrossprod(fit_flash_new_mle$L_pm, fit_flash_new_mle$F_pm)
  B_log1p_new_greedy <- tcrossprod(
    fit_flash_new_greedy$L_pm,
    fit_flash_new_greedy$F_pm
  )

  n_factors_log1p_old <- get_n_factors_flash(fit_flash_old)
  n_factors_log1p_new <- get_n_factors_flash(fit_flash_new_mle)
  n_factors_log1p_new_greedy <- get_n_factors_flash(fit_flash_new_greedy)

  mse_mle <- mean((B_mle - B) ^ 2)
  mse_log1p_old <- mean((B_log1p_old - B) ^ 2)
  mse_log1p_new_mle <- mean((B_log1p_new_mle - B) ^ 2)
  mse_log1p_new_greedy <- mean((B_log1p_new_greedy - B) ^ 2)

  mse_mle_vec <- c(mse_mle_vec, mse_mle)
  mse_log1p_old_vec <- c(mse_log1p_old_vec, mse_log1p_old)
  mse_log1p_new_mle_vec <- c(mse_log1p_new_mle_vec, mse_log1p_new_mle)
  mse_log1p_new_greedy_vec <- c(mse_log1p_new_greedy_vec, mse_log1p_new_greedy)

  factors_log1p_old_vec <- c(factors_log1p_old_vec, n_factors_log1p_old)
  factors_log1p_new_mle_vec <- c(factors_log1p_new_mle_vec, n_factors_log1p_new)
  factors_log1p_new_greedy_vec <- c(factors_log1p_new_greedy_vec, n_factors_log1p_new_greedy)

}

# for now let's suppose that this is correct (though I'm not sure it is)
# then I would like to at least objectively evaluate what I have here

factors_res_df <- data.frame(
  factors = c(factors_log1p_old_vec, factors_log1p_new_mle_vec, factors_log1p_new_greedy_vec),
  method = c(rep("flash_old", 10), rep("flash_new_mle_init", 10), rep("flash_new_greedy_init", 10))
)

library(dplyr)
df_summary <- factors_res_df %>%
  group_by(method) %>%
  summarise(
    mean_factors = mean(factors, na.rm = TRUE),
    sd_factors = sd(factors, na.rm = TRUE)
  )

mse_res_df <- data.frame(
  mse = c(mse_mle_vec, mse_log1p_new_greedy_vec, mse_log1p_new_mle_vec, mse_log1p_old_vec),
  method = c(rep("MLE", 10), rep("flash_new_greedy_init", 10), rep("flash_new_mle_init", 10), rep("flash_old", 10))
)

df_summary2 <- mse_res_df %>%
  group_by(method) %>%
  summarise(
    mean_mse = mean(mse, na.rm = TRUE),
    sd_mse = sd(mse, na.rm = TRUE)
  )
```

```{r}
df_summary <- readr::read_rds("~/Documents/log1p_experiments/output/factor_summary_sim.rds")

df_summary2 <- readr::read_rds("~/Documents/log1p_experiments/output/mse_summary_sim.rds")
```

First, I evaluated the MSE of estimating $H$. The results are below:

```{r}
knitr::kable(df_summary2)
```
Here, "flash_new" refers to Matthew's approximation he proposed recently and "flash_old" refers to the current procedure. Here, we can see that the MLE performs substantially better than all methods, which is followed by the current flash procedure. The proposed flash procedure seems to do quite poorly with a greedy initialization, where it seems to do okay when initialized at the MLE. 

I also computed the mean number of non-zero factors fit by each model. The results are below:

```{r}
knitr::kable(df_summary)
```
Here, we can see that the old flash method typically adds far too many factors. The new method with greedy initialization seems to do the best, where the method initialized with the MLE seems to do okay. The MLE initialized method generally set some of the factors to 0 in the backfit step.

# Conclusion

While ultimately the goal of this project is to improve interpretation of the data which generally comes from visualizations of model fits, I think it is useful to have some objective numbers coming from simulations. Here, it seems like the greedy initialization seems to work quite poorly for Matthew's new proposal. However, things seemed to be reasonably okay with an MLE initialization. I was a bit surprised that the MLE outperformed the other methods in estimating $H$, but I suppose that the other models are mis-specified, so they face an additional disadvantage. I think it is worth understanding qualitative differences in these fits as well, but I will leave that for another vignette. 
