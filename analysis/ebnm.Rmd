---
title: "A short investigation of the normal approximation to the Poisson log1p model"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-07-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ebnm)
```

Here, we consider the following two poisson means problems: 

First, the "log1p link" model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\log(1 + \lambda_{i}) &= b_{i} \\
b_{1}, \dots, b_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{Exponential}(\mu)
\end{align*}

Second, the identity link model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{Exponential}(\mu)
\end{align*}


First, I simulate data from the log1p model with $\pi_{0} = \frac{2}{5}$, $\mu = 2$, and $n = 3,000$.

```{r}
set.seed(1)
n <- 3000
pi0 <- (2/5)
mu <- 2
b <- rexp(n = n, rate = mu)
z <- rbinom(n = n, size = 1, pi0)
b[which(z == 1)] <- 0
lambda <- exp(b) - 1
y <- rpois(n = n, lambda)
```

In Matthew's vignette, for the log1p model, he suggests the following approximation for the likelihood of $b_{i}$ based on a Taylor expansion:

\begin{equation*}
  L(b_{i}) \approx N(\hat{b}(y_{i}), s^{2}(y_{i})),
\end{equation*}

where

\begin{equation*}
  \hat{b}(y_{i}) =
    \begin{cases}
      \log(1 + y_{i}) & \text{if $y_{i} > 0$}\\
      -1 & \text{if $y_{i} = 0$}
    \end{cases}    
\end{equation*}

and

\begin{equation*}
  s^{2}(y_{i}) =
    \begin{cases}
      \frac{y_{i}}{(1 + y_{i})^{2}} & \text{if $y_{i} > 0$}\\
      1 & \text{if $y_{i} = 0$}
    \end{cases}    
\end{equation*}

This approximation is implemented below:

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1
```

Now, we estimate both the ebnm approximation to the log1p model and the identity link model. 

```{r}
get_marginal_lik <- function(y, pi0, mu) {
  
  p <- 1 / (1 + mu)
  lik <- pi0 * ifelse(y == 0, 1, 0) + (1 - pi0) * (1 - p) * (p ^ y)
  total_lik <- sum(log(lik))
  return(total_lik)
  
}

eb_opt_fn <- function(par, y) {
  
  pi0 <- boot::inv.logit(par[1])
  mu <- exp(par[2])
  -get_marginal_lik(y, pi0, mu)
  
}

get_eb_opt <- function(y) {
  
  opt_out <- optim(
    par = rnorm(2),
    fn = eb_opt_fn,
    y = y
  )
  
  pi0_out <- boot::inv.logit(opt_out$par[1])
  mu_out <- exp(opt_out$par[2])
  
  return(
    list(
      pi0 = pi0_out,
      mu = mu_out
    )
  )
  
}

get_pm <- function(y, pi0, mu) {
  
  nz_pm <- (y + 1) / (mu + 1)
  p <- 1 / (1 + mu)
  post_pi0 <- ifelse(
    y == 0,
    pi0 / (
      pi0 + (1 - pi0) * (1 - p)
    ),
    0
  )
  pm <- (1 - post_pi0) * nz_pm
  return(pm)
  
}

solve_pois_mean_id_link <- function(y) {
  
  eb_par <- get_eb_opt(y)
  pm <- get_pm(y, eb_par$pi0, eb_par$mu)
  return(
    list(
      pi0 = eb_par$pi0,
      mu = eb_par$mu,
      pm = pm
    )
  )
  
}

out <- ebnm(x = b_hat, s = sqrt(s2), prior_family = "point_exponential")
lambda_pm <- exp(out$posterior$mean) - 1
id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b = b,
  b_hat = log1p(y)
)
```

Now, it's useful to compare the posterior mean estimates of lambda from the different models.

```{r, fig.width=8, fig.height=8}
library(ggplot2)
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

library(glue)
print(glue("MAE of log1p model: {mae_log1p}"))
print(glue("MAE of id model: {mae_id}"))
print(glue("MSE of log1p model: {mse_log1p}"))
print(glue("MSE of id model: {mse_id}"))
```

In this case, it seems like the identity link undershrinks for small values of $y$, where it overshrinks for large values of $y$. It is reassuring that the approximation to the true model at least outperforms the wrong model in this simple scenario. 

Now, I'd like to simulate data such that both models introduced above are mis-specified and compare the performance of the different fitting procedures.

Below, I simulate data from the following model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \chi^{2}_{4}
\end{align*}

with $\pi_{0} = \frac{2}{5}$. Below are the posterior means fitting the two models described above:

```{r}
set.seed(10)
n <- 3000
pi0 <- (2/5)
lambda <- rchisq(n = n, df = 4)
z <- rbinom(n = n, size = 1, pi0)
lambda[which(z == 1)] <- 0
y <- rpois(n = n, lambda)
```

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1

out <- ebnm(x = b_hat, s = sqrt(s2), prior_family = "point_exponential")
lambda_pm <- exp(out$posterior$mean) - 1
id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b = b,
  b_hat = log1p(y)
)
```

```{r, fig.width=8, fig.height=8}
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

print(glue("MAE of log1p model: {mae_log1p}"))
print(glue("MAE of id model: {mae_id}"))
print(glue("MSE of log1p model: {mse_log1p}"))
print(glue("MSE of id model: {mse_id}"))
```

Here, it seems that the identity link performs better, but the advantage is slight. Changing random seeds generally doesn't seem to change the results greatly.

Finally, I simulate data from the following model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{LogNormal}(0, 1.25)
\end{align*}

with $\pi_{0} = \frac{1}{2}$. Below are the results:

```{r}
set.seed(10)
n <- 3000
pi0 <- (1/2)
lambda <- exp(rnorm(n = n, mean = 0, sd = sqrt(1.25)))
z <- rbinom(n = n, size = 1, pi0)
lambda[which(z == 1)] <- 0
y <- rpois(n = n, lambda)
```

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1

out <- ebnm(x = b_hat, s = sqrt(s2), prior_family = "point_exponential")
lambda_pm <- exp(out$posterior$mean) - 1
id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b = b,
  b_hat = log1p(y)
)
```

```{r, fig.width=8, fig.height=8}
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

print(glue("MAE of log1p model: {mae_log1p}"))
print(glue("MAE of id model: {mae_id}"))
print(glue("MSE of log1p model: {mse_log1p}"))
print(glue("MSE of id model: {mse_id}"))
```

Here, we see that the log1p model performs slightly better.

# Conclusion

Overall, for the task of finding posterior means, it seems like Matthew's approximation performs reasonably well when compared to the Poisson means model with identity link.
