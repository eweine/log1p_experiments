---
title: "A short investigation of the normal approximation to the Poisson log1p model"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-07-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ebnm)
```

Here, we consider the following two poisson means problems: 

First, the "log1p link" model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\log(1 + \lambda_{i}) &= b_{i} \\
b_{1}, \dots, b_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{Exponential}(\mu)
\end{align*}

Second, the identity link model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{Exponential}(\mu)
\end{align*}


First, I simulate data from the log1p model with $\pi_{0} = \frac{2}{5}$, $\mu = 2$, and $n = 3,000$.

```{r}
set.seed(1)
n <- 3000
pi0 <- (2/5)
mu <- 2
b <- rexp(n = n, rate = mu)
z <- rbinom(n = n, size = 1, pi0)
b[which(z == 1)] <- 0
lambda <- exp(b) - 1
y <- rpois(n = n, lambda)
```

In Matthew's vignette, for the log1p model, he suggests the following approximation for the likelihood of $b_{i}$ based on a Taylor expansion:

\begin{equation*}
  L(b_{i}) \approx N(\hat{b}(y_{i}), s^{2}(y_{i})),
\end{equation*}

where

\begin{equation*}
  \hat{b}(y_{i}) =
    \begin{cases}
      \log(1 + y_{i}) & \text{if $y_{i} > 0$}\\
      -1 & \text{if $y_{i} = 0$}
    \end{cases}    
\end{equation*}

and

\begin{equation*}
  s^{2}(y_{i}) =
    \begin{cases}
      \frac{y_{i}}{(1 + y_{i})^{2}} & \text{if $y_{i} > 0$}\\
      1 & \text{if $y_{i} = 0$}
    \end{cases}    
\end{equation*}

This approximation is implemented below:

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1
```

Now, we estimate both the ebnm approximation to the log1p model and the identity link model. 

```{r}
get_marginal_lik <- function(y, pi0, mu) {
  
  p <- 1 / (1 + mu)
  lik <- pi0 * ifelse(y == 0, 1, 0) + (1 - pi0) * (1 - p) * (p ^ y)
  total_lik <- sum(log(lik))
  return(total_lik)
  
}

eb_opt_fn <- function(par, y) {
  
  pi0 <- boot::inv.logit(par[1])
  mu <- exp(par[2])
  -get_marginal_lik(y, pi0, mu)
  
}

get_eb_opt <- function(y) {
  
  opt_out <- optim(
    par = rnorm(2),
    fn = eb_opt_fn,
    y = y
  )
  
  pi0_out <- boot::inv.logit(opt_out$par[1])
  mu_out <- exp(opt_out$par[2])
  
  return(
    list(
      pi0 = pi0_out,
      mu = mu_out
    )
  )
  
}

get_pm <- function(y, pi0, mu) {
  
  nz_pm <- (y + 1) / (mu + 1)
  p <- 1 / (1 + mu)
  post_pi0 <- ifelse(
    y == 0,
    pi0 / (
      pi0 + (1 - pi0) * (1 - p)
    ),
    0
  )
  pm <- (1 - post_pi0) * nz_pm
  return(pm)
  
}

get_pm_b <- function(y, pi0, mu, nsamps = 5000) {
  
  p <- 1 / (1 + mu)
  post_pi0 <- ifelse(
    y == 0,
    pi0 / (
      pi0 + (1 - pi0) * (1 - p)
    ),
    0
  )
  
  # now, I want to take posterior samples to get the posterior mean of b
  pi0_samps <- rbinom(
    n = length(y) * nsamps,
    size = 1,
    prob = post_pi0
  )
  
  lambda_nz_samps <- rgamma(
    n = length(y) * nsamps,
    shape = y + 1,
    rate = 1 + mu
  )
  
  lambda_samps <- (1 - pi0_samps) * lambda_nz_samps
  lambda_mat <- matrix(
    data = lambda_samps,
    nrow = nsamps,
    byrow = TRUE
  )
  
  b_mat <- log1p(lambda_mat)
  b_pm <- colMeans(b_mat)
  return(b_pm)
  
}

solve_pois_mean_id_link <- function(y) {
  
  eb_par <- get_eb_opt(y)
  pm <- get_pm(y, eb_par$pi0, eb_par$mu)
  pm_b <- get_pm_b(y, eb_par$pi0, eb_par$mu)
  return(
    list(
      pi0 = eb_par$pi0,
      mu = eb_par$mu,
      pm = pm,
      pm_b = pm_b
    )
  )
  
}

out <- ebnm(
  x = b_hat, 
  s = sqrt(s2), 
  prior_family = "point_exponential", 
  output = ebnm_output_all()
)

post_samps <- out$posterior_sampler(5000)
lambda_samps <- exp(post_samps) - 1
lambda_pm <- colMeans(lambda_samps)

id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b_pm_id = id_link_out$pm_b,
  b = b,
  b_hat = log1p(y)
)
```

Now, it's useful to compare the posterior mean estimates of lambda from the different models.

```{r, fig.width=8, fig.height=8}
library(ggplot2)
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

We can also compare the posterior means of $b$ in the log1p model above.

```{r, fig.width=8, fig.height=8}
library(ggplot2)
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ ID link") +
  xlab("b")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ log1p link") +
  xlab("b")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = b_pm_id, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of b w/ log1p link") +
  ylab("Posterior mean of b w/ ID link")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

library(glue)
print(glue("MAE for lambda of log1p model: {mae_log1p}"))
print(glue("MAE for lambda of id model: {mae_id}"))
print(glue("MSE for lambda of log1p model: {mse_log1p}"))
print(glue("MSE for lambda of id model: {mse_id}"))

mae_log1p <- mean(abs(df_out$b_pm - df_out$b))
mae_id <- mean(abs(df_out$b_pm_id - df_out$b))
mse_log1p <- mean((df_out$b_pm - df_out$b) ^ 2)
mse_id <- mean((df_out$b_pm_id - df_out$b) ^ 2)

print(glue("MAE for b of log1p model: {mae_log1p}"))
print(glue("MAE for b of id model: {mae_id}"))
print(glue("MSE for b of log1p model: {mse_log1p}"))
print(glue("MSE for b of id model: {mse_id}"))
```

In this case, it seems like the identity link undershrinks for small values of $y$, where it overshrinks for large values of $y$. However, the id model actually performs slightly better in estimating $b$.

Now, I'd like to simulate data such that both models introduced above are mis-specified and compare the performance of the different fitting procedures.

Below, I simulate data from the following model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \chi^{2}_{4}
\end{align*}

with $\pi_{0} = \frac{2}{5}$. Below are the posterior means of lambda derived from fitting the two models described above:

```{r}
set.seed(10)
n <- 3000
pi0 <- (2/5)
lambda <- rchisq(n = n, df = 4)
z <- rbinom(n = n, size = 1, pi0)
lambda[which(z == 1)] <- 0
b <- log1p(lambda)
y <- rpois(n = n, lambda)
```

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1

out <- ebnm(x = b_hat, s = sqrt(s2), prior_family = "point_exponential", output = ebnm_output_all())
post_samps <- out$posterior_sampler(5000)
lambda_samps <- exp(post_samps) - 1
lambda_pm <- colMeans(lambda_samps)
id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b_pm_id = id_link_out$pm_b,
  b = b,
  b_hat = log1p(y)
)
```

```{r, fig.width=8, fig.height=8}
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

We can also get the posterior means of b:

```{r, fig.width=8, fig.height=8}
library(ggplot2)
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ ID link") +
  xlab("b")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ log1p link") +
  xlab("b")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = b_pm_id, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of b w/ log1p link") +
  ylab("Posterior mean of b w/ ID link")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

library(glue)
print(glue("MAE for lambda of log1p model: {mae_log1p}"))
print(glue("MAE for lambda of id model: {mae_id}"))
print(glue("MSE for lambda of log1p model: {mse_log1p}"))
print(glue("MSE for lambda of id model: {mse_id}"))

mae_log1p <- mean(abs(df_out$b_pm - df_out$b))
mae_id <- mean(abs(df_out$b_pm_id - df_out$b))
mse_log1p <- mean((df_out$b_pm - df_out$b) ^ 2)
mse_id <- mean((df_out$b_pm_id - df_out$b) ^ 2)

print(glue("MAE for b of log1p model: {mae_log1p}"))
print(glue("MAE for b of id model: {mae_id}"))
print(glue("MSE for b of log1p model: {mse_log1p}"))
print(glue("MSE for b of id model: {mse_id}"))
```

Here, it seems that the identity link performs better, but the advantage is slight. Changing random seeds generally doesn't seem to change the results greatly.

Finally, I simulate data from the following model:

\begin{align*}
y_{i} &\sim \text{Poisson}(\lambda_{i}) \\
\lambda_{1}, \dots, \lambda_{n} & \overset{\text{iid}}{\sim} \pi_{0} \delta_{0} + (1 - \pi_{0}) \cdot \text{LogNormal}(0, 1.25)
\end{align*}

with $\pi_{0} = \frac{1}{2}$. Below are the results:

```{r}
set.seed(10)
n <- 3000
pi0 <- (1/2)
lambda <- exp(rnorm(n = n, mean = 0, sd = sqrt(1.25)))
z <- rbinom(n = n, size = 1, pi0)
lambda[which(z == 1)] <- 0
y <- rpois(n = n, lambda)
b <- log1p(lambda)
```

```{r}
b_hat <- log1p(y)
b_hat[which(y == 0)] <- -1
s2 <- y / ((1 + y) ^ 2)
s2[which(y == 0)] <- 1

out <- ebnm(x = b_hat, s = sqrt(s2), prior_family = "point_exponential", output = ebnm_output_all())
post_samps <- out$posterior_sampler(5000)
lambda_samps <- exp(post_samps) - 1
lambda_pm <- colMeans(lambda_samps)
id_link_out <- solve_pois_mean_id_link(y)
df_out <- data.frame(
  y = y,
  lambda_pm = lambda_pm,
  lambda_pm_id = id_link_out$pm,
  lambda = lambda,
  b_pm = out$posterior$mean,
  b_pm_id = id_link_out$pm_b,
  b = b,
  b_hat = log1p(y)
)
```

```{r, fig.width=8, fig.height=8}
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ ID link") +
  xlab("lambda")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of lambda w/ log1p link") +
  xlab("lambda")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = lambda_pm_id, y = lambda_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of lambda w/ log1p link") +
  ylab("Posterior mean of lambda w/ ID link")

ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

We can also get the posterior mean of b:

```{r, fig.width=8, fig.height=8}
library(ggplot2)
g1 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm_id)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ ID link") +
  xlab("b")

g2 <- ggplot(data = df_out) +
  geom_point(aes(x = b, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  ylab("Posterior mean of b w/ log1p link") +
  xlab("b")

g3 <- ggplot(data = df_out) +
  geom_point(aes(x = b_pm_id, y = b_pm)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  cowplot::theme_cowplot() +
  xlab("Posterior mean of b w/ log1p link") +
  ylab("Posterior mean of b w/ ID link")

library(ggpubr)
ggarrange(g1, g2, g3, nrow = 2, ncol = 2)
```

```{r}
mae_log1p <- mean(abs(df_out$lambda_pm - df_out$lambda))
mae_id <- mean(abs(df_out$lambda_pm_id - df_out$lambda))
mse_log1p <- mean((df_out$lambda_pm - df_out$lambda) ^ 2)
mse_id <- mean((df_out$lambda_pm_id - df_out$lambda) ^ 2)

library(glue)
print(glue("MAE for lambda of log1p model: {mae_log1p}"))
print(glue("MAE for lambda of id model: {mae_id}"))
print(glue("MSE for lambda of log1p model: {mse_log1p}"))
print(glue("MSE for lambda of id model: {mse_id}"))

mae_log1p <- mean(abs(df_out$b_pm - df_out$b))
mae_id <- mean(abs(df_out$b_pm_id - df_out$b))
mse_log1p <- mean((df_out$b_pm - df_out$b) ^ 2)
mse_id <- mean((df_out$b_pm_id - df_out$b) ^ 2)

print(glue("MAE for b of log1p model: {mae_log1p}"))
print(glue("MAE for b of id model: {mae_id}"))
print(glue("MSE for b of log1p model: {mse_log1p}"))
print(glue("MSE for b of id model: {mse_id}"))
```

Here, we see that the log1p model performs slightly better for estimating $\lambda$ but slightly worse for estimating $b$.

# Conclusion

Overall, for the task of finding posterior means, it seems like Matthew's approximation performs reasonably well when compared to the Poisson means model with identity link.
