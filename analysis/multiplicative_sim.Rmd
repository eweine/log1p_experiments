---
title: "Simulations with Multiplicative Effects"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-09-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(ggplot2)
library(fastTopics)
library(log1pNMF)
library(ggh4x)
library(ggpubr)
library(dplyr)
library(fastglmpca)
```

## Introduction

Previously, we have seen that the topic model is more likely to cluster groups, where the log1p model is more likely to give a parts based representation of the data. One intuition for why this might be happening is that if there are truly multiplicative effects in the data, the topic model may not be able to represent them well because the factors are additive. Below, I explore a simulation scenario where this appears to be the case.

## Simulation

I simulated expression values for $5$ different groups, where $80\%$ of genes were not expressed at all in each group and the remaining $20\%$ had mean expression sampled from an exponential distribution with rate $1$ (note that the expressed genes were mutually exclusive between groups). Then, for each group I created two subgroups, where in one subgroup the expression is as described above, and in the other subgroup a random subset of the expressed genes is expressed at $\exp(b + 2.5)$, where $b$ is the baseline expression in the group. 

I then fit both the topic model and the log1p model with $c = 1$ to this data. 

```{r}
# I think that the next thing I should try is to do more direct
# multiplication instead of using the link function
n_cells <- 1000

grouping <- c(
  rep("A1", 100), rep("A2", 100),
  rep("B1", 100), rep("B2", 100),
  rep("C1", 100), rep("C2", 100),
  rep("D1", 100), rep("D2", 100),
  rep("E1", 100), rep("E2", 100)
)

set.seed(1)
lambda_group1_c <- c(rexp(200), rep(0, 800))
lambda_group2_c <- c(rep(0, 200), rexp(200), rep(0, 600))
lambda_group3_c <- c(rep(0, 400), rexp(200), rep(0, 400))
lambda_group4_c <- c(rep(0, 600), rexp(200), rep(0, 200))
lambda_group5_c <- c(rep(0, 800), rexp(200))

mult_vec <- rep(2.5, 1000)
mult_vec[sample(1:1000, size = 800)] <- 0

lambda_group1_t <- if_else(
  mult_vec == 0,
  lambda_group1_c,
  exp(mult_vec + lambda_group1_c)
)

lambda_group2_t <- if_else(
  mult_vec == 0,
  lambda_group2_c,
  exp(mult_vec + lambda_group2_c)
)

lambda_group3_t <- if_else(
  mult_vec == 0,
  lambda_group3_c,
  exp(mult_vec + lambda_group3_c)
)

lambda_group4_t <- if_else(
  mult_vec == 0,
  lambda_group4_c,
  exp(mult_vec + lambda_group4_c)
)

lambda_group5_t <- if_else(
  mult_vec == 0,
  lambda_group4_c,
  exp(mult_vec + lambda_group5_c)
)

lambda_list <- list()
Y_list <- list()

lambda_list[[1]] <- lambda_group1_c
lambda_list[[2]] <- lambda_group1_t
lambda_list[[3]] <- lambda_group2_c
lambda_list[[4]] <- lambda_group2_t
lambda_list[[5]] <- lambda_group3_c
lambda_list[[6]] <- lambda_group3_t
lambda_list[[7]] <- lambda_group4_c
lambda_list[[8]] <- lambda_group4_t
lambda_list[[9]] <- lambda_group5_c
lambda_list[[10]] <- lambda_group5_t

set.seed(1)
n_cells_per_group <- 100
n_genes <- 1000

for (group in 1:10) {
  
  Lambda <- matrix(
    data = rep(lambda_list[[group]], n_cells_per_group),
    nrow = n_cells_per_group,
    ncol = n_genes,
    byrow = TRUE
  )
  
  Y <- matrix(
    data = rpois(n = n_cells_per_group * n_genes, lambda = as.vector(Lambda)),
    nrow = n_cells_per_group,
    ncol = n_genes
  )
  
  Y_list[[group]] <- Y
  
}

Y <- do.call(rbind, Y_list)

Y <- as(Y, "CsparseMatrix")
Y <- Y[,Matrix::colSums(Y) > 0]

library(fastTopics)

ft_r1 <- fastTopics:::fit_pnmf_rank1(Y)

init_LL <- cbind(
  ft_r1$L,
  matrix(data = 1e-5, nrow = nrow(Y), ncol = 5)
)

init_FF <- cbind(
  ft_r1$F,
  matrix(data = 1e-5, nrow = ncol(Y), ncol = 5)
)

ft_init <- init_poisson_nmf(X = Y, F = init_FF, L = init_LL)

ft_r1_init <- fit_poisson_nmf(
  X = Y, 
  fit0 = ft_init, 
  control = list(nc = 7),
  verbose = "none"
)
sp1 <- structure_plot(ft_r1_init, loadings_order = 1:n_cells, grouping = grouping, gap = 10) +
  theme(axis.text.x = element_text(angle = 0,hjust = 0.5, size = 12)) + ylab("Membership") + ggtitle("Topic Model")

log1p_fit_list <- list()
cc_vec <- c(1e-3, 1e-2, 1e-1, 1, 10, 100, 1000)

for (cc in cc_vec) {
  
  set.seed(1)
  log1p_fit_list[[as.character(cc)]] <- fit_poisson_log1p_nmf(
    Y = Y, K = 6, loglik = "exact", init_method = "rank1",
    control = list(maxiter = 250, verbose = FALSE), cc = cc
  )
  
}


sp2 <- normalized_structure_plot(log1p_fit_list[[as.character(1)]], loadings_order = 1:n_cells, grouping = grouping, gap = 10, topics = c(1, rev(2:6))) +
  theme(axis.text.x = element_text(angle = 0,hjust = 0.5, size = 12)) + ylab("Membership") + ggtitle("log1p Model (c = 1)")

sp3 <- normalized_structure_plot(log1p_fit_list[[as.character(0.001)]], loadings_order = 1:n_cells, grouping = grouping, gap = 10, topics = c(1, rev(2:6))) +
  theme(axis.text.x = element_text(angle = 0,hjust = 0.5, size = 12)) + ylab("Membership") + ggtitle("log1p Model (c = 1e-3)")

```

```{r}
ggarrange(sp1, sp2, sp3, nrow = 3, ncol = 1)
```

```{r}
hoyer_sparsity <- function(x) {
  
  n <- length(x)
  (1 / (sqrt(n) - 1)) * (sqrt(n) - (sum(x) / (sqrt(sum(x ^ 2)))))
  
}

for (cc in cc_vec) {
  
  log1p_fit_list[[as.character(cc)]]$l_sparsity <- apply(
    log1p_fit_list[[as.character(cc)]]$LL, 2, hoyer_sparsity
  )
  
  log1p_fit_list[[as.character(cc)]]$f_sparsity <- apply(
    log1p_fit_list[[as.character(cc)]]$FF, 2, hoyer_sparsity
  )
  
}

l_sparsity_vec <- unlist(lapply(log1p_fit_list, function(x) {mean(x$l_sparsity)}))

f_sparsity_vec <- unlist(lapply(log1p_fit_list, function(x) {mean(x$f_sparsity)}))

s <- Matrix::rowSums(Y)
s <- s / mean(s)

l_tm_sparsity <- mean(apply(diag(1/s) %*% ft_r1_init$L, 2, hoyer_sparsity))
f_tm_sparsity <- mean(apply(ft_r1_init$F, 2, hoyer_sparsity))

df_sparsity_l <- data.frame(
  cc = cc_vec,
  sparsity = l_sparsity_vec
)

ggplot(data = df_sparsity_l, aes(x = cc, y = sparsity)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1e-3, 1, 1e3), transform = "log10") +
  xlab("c") +
  ylab("Mean Loading Sparsity") +
  geom_hline(yintercept = l_tm_sparsity, color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.006, y=l_tm_sparsity + 0.05, label="Topic Model", color="red"
  )

df_sparsity_f <- data.frame(
  cc = cc_vec,
  sparsity = f_sparsity_vec
)

ggplot(data = df_sparsity_f, aes(x = cc, y = sparsity)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1e-3, 1, 1e3), transform = "log10") +
  xlab("c") +
  ylab("Mean Loading Sparsity") +
  geom_hline(yintercept = f_tm_sparsity, color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.006, y=f_tm_sparsity + 0.05, label="Topic Model", color="red"
  )
```


While neither model captures the structure perfectly (which is expected because I'm not generating data from a log1p or topic model), it is clear that the log1p model is much closer. The topic model, on the other hand, just completely clusters the groups.

Finally, I wanted to know what GLM-PCA would do with this simulation. Below is a plot of the first two PCs:

```{r, message=FALSE, results='hide'}
set.seed(1)
fgpca_fit <- fit_glmpca_pois(Y = Matrix::t(Y), K = 2, control = list(maxiter = 350))
gpca_df <- as.data.frame(fgpca_fit$V)
gpca_df$grouping <- grouping
ggplot(data = gpca_df, aes(x = k_1, y = k_2)) +
  geom_point(aes(color = grouping)) +
  cowplot::theme_cowplot() +
  scale_color_manual(values = fastTopics:::kelly()[2:11]) +
  xlab("PC1") +
  ylab("PC2")
```

It's interesting that in only two dimensions you are able to get clear clusters, but you do not get a very parts based representation.