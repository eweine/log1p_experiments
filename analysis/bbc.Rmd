---
title: "BBC Article Analysis"
output:
  workflowr::wflow_html:
    code_folding: hide
date: "2024-11-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

Here, I will apply the `log1p` matrix factorization model to a dataset of approximately $2K$ BBC news articles. To pre-process the data, I perform stemming and remove stop words using the package `tm`. I fit the model without using any approximation for various values of $c$. I also used a size factor on the order of $1$ that is proportional to the total number of words in each article. Finally, to initialize each fit, I first fit a $K = 1$ model to convergence and then initialize the $K = 30$ fit with small positive numbers in factors $2-29$ and the rank 1 fit for the first factor.


```{r}
dat <- readr::read_csv("~/Downloads/bbc_news_text_complexity_summarization.csv")

library(tm)
library(SnowballC)

my_corpus <- VCorpus(VectorSource(dat$text))

addspace <- content_transformer(function(x, pattern) {
  return(gsub(pattern, " ", x))
})

my_corpus <- tm_map(my_corpus, addspace, "-")

my_corpus <- tm_map(my_corpus, removeNumbers)

# Transform to lower case (need to wrap in content_transformer)
my_corpus <- tm_map(my_corpus,content_transformer(tolower))
my_corpus <- tm_map(my_corpus, removeWords, stopwords("SMART"))
my_corpus <- tm_map(my_corpus, removePunctuation)
my_corpus <- tm_map(my_corpus, stripWhitespace)

my_corpus <- tm_map(my_corpus, stemDocument)

dtm <- DocumentTermMatrix(my_corpus)
dtm2 <- Matrix::sparseMatrix(
  i = dtm$i,
  j = dtm$j,
  x = dtm$v
)


colnames(dtm2) <- dtm$dimnames$Terms
words_to_use <- which(Matrix::colSums(dtm2>0)>4)

dtm2 <- dtm2[,words_to_use]

s <- Matrix::rowSums(dtm2)
s <- s / mean(s)

library(passPCA)
library(Matrix)
library(dplyr)
cc_vec <- c(0.0001, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4)
```

```{r, eval=FALSE}
fit_list <- list()
n <- nrow(dtm2)
p <- ncol(dtm2)

K <- 30

for (cc in cc_vec) {
  
  print(cc)
  
  set.seed(1)
  log1p_k1 <- fit_factor_model_log1p_exact(
    Y = dtm2,
    K = 1,
    maxiter = 10,
    s = cc * s,
    init_method = "frob_nmf"
  )
  
  init_LL <- log1p_k1$U %>%
    cbind(
      matrix(
        data = rexp(
          n = n * (K - 1), rate = 15
        ),
        nrow = n,
        ncol = K - 1
      )
    )
  
  init_FF <- log1p_k1$V %>%
    cbind(
      matrix(
        data = rexp(
          n = p * (K - 1), rate = 15
        ),
        nrow = p,
        ncol = K - 1
      )
    )
  
  
  set.seed(1)
  fit <- fit_factor_model_log1p_exact(
    Y = dtm2,
    K = K,
    init_U = init_LL,
    init_V = init_FF,
    maxiter = 100,
    s = cc * s
  )
  
  fit_list[[as.character(cc)]] <- fit
  
}

library(fastTopics)
nmf <- fit_poisson_nmf(X = dtm2, k = K, control = list(nc = 6))

fit_list[["nmf"]] <- nmf
```

```{r}
fit_list <- readr::read_rds(
  "~/Documents/data/passPCA/bbc_articles.rds"
  )
```

# Analysis of fits
## Log-likelihood

Below is a plot of the log-likelihoods of the data under a Poisson model using different values of $c$. The dashed red line is the log-likelihood of the topic model as fit via `fastTopics`. Surprisingly, very small values of $c$ actually lead to the highest likelihood by far. 

```{r}
ll_vec <- c()

for (cc in cc_vec) {

  fit <- fit_list[[as.character(cc)]] 
  B <- fit$U %*% t(fit$V)
  Lambda <- cc * (exp(B) - 1)
  Lambda <- as.matrix(Matrix::Diagonal(x = s) %*% Lambda)
  
  ll <- sum(
    dpois(
      x = as.vector(as.matrix(dtm2)),
      lambda = as.vector(Lambda),
      log = TRUE
    )
  )
  
  ll_vec <- c(ll_vec, ll)
  
}

ll_nmf <- sum(
  dpois(
    x = as.vector(as.matrix(dtm2)),
    lambda = as.vector(fit_list$nmf$L %*% t(fit_list$nmf$F)),
    log = TRUE
  )
)
```

```{r}
df <- data.frame(
  cc = cc_vec,
  ll_diff = max(ll_vec) - ll_vec + 1
)

library(ggplot2)
ggplot(data = df) +
  geom_point(aes(x = cc, y = ll_diff)) +
  geom_line(aes(x = cc, y = ll_diff)) +
  xlab("c") +
  ylab("Distance from best log-likelihood") +
  scale_x_log10() +
  scale_y_log10() +
  geom_hline(yintercept = max(ll_vec) - ll_nmf + 1, color = "red", linetype = "dashed")
```

## Loadings

Each article in the original dataset comes with a section label (one of business, entertainment, politics, sports, or tech). I'm interested in understanding how the loadings for different articles compare across article category. 

```{r}
normalize_bars <- function(LL) {

  max_col <- apply(LL, 2, max)
  sweep(LL, 2, max_col, FUN = "/")

}
```


```{r, fig.width=12, fig.height=33}
library(tidyr)
library(stringi)
library(stringr)
cell.type <- as.factor(dat$labels)

LL_c0001 <- normalize_bars(fit_list$`1e-04`$U)

# Downsample the number of cells and sort them using tSNE.
set.seed(8675309)
cell.idx <- numeric(0)
cell.types <- levels(cell.type)
for (i in 1:length(cell.types)) {
  which.idx <- which(cell.type == cell.types[i])
  # Downsample common cell types.
  if (length(which.idx) > 1000) {
    which.idx <- sample(which.idx, 1000)
  }
  # Don't include rare cell types.
  if (length(which.idx) > 10) {
    # Sort using tsne.
    tsne.res <- Rtsne::Rtsne(
      LL_c0001[which.idx, ],
      dims = 1,
      pca = FALSE,
      normalize = FALSE,
      perplexity = min(100, floor((length(which.idx) - 1) / 3) - 1),
      theta = 0.1,
      max_iter = 1000,
      eta = 200,
      check_duplicates = FALSE
    )$Y[, 1]
    which.idx <- which.idx[order(tsne.res)]
    cell.idx <- c(cell.idx, which.idx)
  }
}

cell.type <- cell.type[cell.idx]
cell.type <- droplevels(cell.type)

LL_c0001 <- LL_c0001[cell.idx, ]
LL_c001 <- normalize_bars(fit_list$`0.001`$U)
LL_c001 <- LL_c001[cell.idx, ]
LL_c01 <- normalize_bars(fit_list$`0.01`$U)
LL_c01 <- LL_c01[cell.idx, ]
LL_cp1 <- normalize_bars(fit_list$`0.1`$U)
LL_cp1 <- LL_cp1[cell.idx, ]
LL_c1 <- normalize_bars(fit_list$`1`$U)
LL_c1 <- LL_c1[cell.idx, ]
LL_c10 <- normalize_bars(fit_list$`10`$U)
LL_c10 <- LL_c10[cell.idx, ]
LL_c100 <- normalize_bars(fit_list$`100`$U)
LL_c100 <- LL_c100[cell.idx, ]
LL_c1000 <- normalize_bars(fit_list$`1000`$U)
LL_c1000 <- LL_c1000[cell.idx, ]
LL_cinf <- fit_list$nmf$L
LL_cinf <- LL_cinf[cell.idx, ]

make.heatmap.tib <- function(FF) {
  tib <- as_tibble(scale(FF, center = FALSE, scale = apply(FF, 2, max))) %>%
    mutate(Cell.type = cell.type) %>%
    arrange(Cell.type) %>%
    mutate(Cell.idx = row_number())
  
  tib <- tib %>%
    pivot_longer(
      -c(Cell.idx, Cell.type),
      names_to = "Factor",
      values_to = "Loading",
      values_drop_na = TRUE
    ) %>%
    mutate(Factor = as.numeric(str_extract(Factor, "[0-9]+")))
  
  return(tib)
}

LL_c0001_tib <- make.heatmap.tib(LL_c0001)
LL_c001_tib <- make.heatmap.tib(LL_c001)
LL_c01_tib <- make.heatmap.tib(LL_c01)
LL_cp1_tib <- make.heatmap.tib(LL_cp1)
LL_c1_tib <- make.heatmap.tib(LL_c1)
LL_c10_tib <- make.heatmap.tib(LL_c10)
LL_c100_tib <- make.heatmap.tib(LL_c100)
LL_c1000_tib <- make.heatmap.tib(LL_c1000)
LL_cinf_tib <- make.heatmap.tib(LL_cinf)

heatmap.tib <- LL_c0001_tib %>% mutate(Method = "c = 0.0001") %>%
  bind_rows(LL_c001_tib %>% mutate(Method = "c = 0.001")) %>%
  bind_rows(LL_c01_tib %>% mutate(Method = "c = 0.01")) %>%
  bind_rows(LL_cp1_tib %>% mutate(Method = "c = 0.1")) %>%
  bind_rows(LL_c1_tib %>% mutate(Method = "c = 1")) %>%
  bind_rows(LL_c10_tib %>% mutate(Method = "c = 10")) %>%
  bind_rows(LL_c100_tib %>% mutate(Method = "c = 100")) %>%
  bind_rows(LL_c1000_tib %>% mutate(Method = "c = 1000")) %>%
  bind_rows(LL_cinf_tib %>% mutate(Method = "Topic Model")) %>% 
  mutate(Method = factor(Method, levels = c("c = 0.0001", "c = 0.001", "c = 0.01", "c = 0.1", "c = 1", "c = 10", "c = 100", "c = 1000", "Topic Model")))

tib <- heatmap.tib %>%
  group_by(Cell.type, Cell.idx) %>%
  summarize()

cell_type_breaks <- c(1, which(tib$Cell.type[-1] != tib$Cell.type[-nrow(tib)]))
label_pos <- cell_type_breaks / 2 + c(cell_type_breaks[-1], nrow(tib)) / 2

library(ggplot2)

plt <- ggplot(heatmap.tib, aes(x = Factor, y = -Cell.idx, fill = Loading)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "firebrick") +
  labs(y = "") +
  scale_y_continuous(breaks = -label_pos,
                     minor_breaks = NULL,
                     labels = levels(cell.type)) +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  theme_minimal() +
  geom_hline(yintercept = -cell_type_breaks, size = 0.1) +
  facet_wrap(~Method, ncol = 1, axes = "all") +
  theme(legend.position = "none", 
        strip.text = element_text(size = 16)) 

plt
```

## Factors

Below, I print out the top $10$ words for each factor from each model.

```{r}
get_keywords <- function(V) {
  
  kw_list <- list()
  
  for (k in 1:ncol(V)) {
    
    kw_list[[k]] <- names(head(sort(V[,k], decreasing = T), 10))
    
  }
  
  return(kw_list)
  
}
```

```{r}
kw_lists <- list()

for (cc in c(cc_vec)) {
  
  fit <- fit_list[[as.character(cc)]]
  rownames(fit$V) <- colnames(dtm2)
  
  kw_lists[[as.character(cc)]] <- get_keywords(fit$V)

}

kw_lists[["nmf"]] <- get_keywords(fit_list$nmf$F)
```

### c = 1e-4

```{r}
print(kw_lists[[as.character(1e-4)]])
```

### c = 1e-3

```{r}
print(kw_lists[[as.character(1e-3)]])
```

### c = 1e-2

```{r}
print(kw_lists[[as.character(1e-2)]])
```

### c = 1e-1

```{r}
print(kw_lists[[as.character(1e-1)]])
```

### c = 1

```{r}
print(kw_lists[[as.character(1)]])
```

### c = 10

```{r}
print(kw_lists[[as.character(10)]])
```

### c = 100

```{r}
print(kw_lists[[as.character(100)]])
```

### c = 1000

```{r}
print(kw_lists[[as.character(1000)]])
```

### nmf

```{r}
print(kw_lists[["nmf"]])
```
