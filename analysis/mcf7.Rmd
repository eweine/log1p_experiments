---
title: The MCF-7 data set as possible model system for the Poisson log1p NMF model
author: Eric Weine and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

Here I explore the possibility of the data from the
[Sanford et al eLife paper][sanford2020elife] as a possible "model
system" for the Poisson log1p NMF model. This paper analyzed the
transcriptional response of human MCF-7 cells to retinoic acid and
TGF-$\beta$, applied individually and in combination.

The following abbreviations were used in their data:
EtOH = ethanol; RA = retinoic acid; TGFb = TGF-$\beta$.

Note that the data were downloaded from GEO, accession GSE152749.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the fastTopics and flashier packages, and a few other packages
needed to retrieve the data and create the plots.

```{r load-pkgs, message=FALSE}
library(rsvd)
library(fastglmpca)
library(fastTopics)
library(flashier)
library(data.table)
library(GEOquery)
library(ggplot2)
library(cowplot)
library(log1pNMF)
```

Set the seed for reproducibility:

```{r set-seed}
set.seed(1)
```

## Prepare the data for analysis with fastTopics and flashier

Load the gene information:

```{r load-genes}
genes <- fread("data/Human.GRCh38.p13.annot.tsv.gz",
               sep = "\t",header = TRUE,stringsAsFactors = FALSE)
class(genes) <- "data.frame"
genes <- genes[1:10]
genes <- transform(genes,
                   GeneType = factor(GeneType),
				   Status   = factor(Status))
```

Load the RNA-seq counts:

```{r load-rnaseq}
counts <- fread("data/GSE152749_raw_counts_GRCh38.p13_NCBI.tsv.gz",
                sep = "\t",header = TRUE,stringsAsFactors = FALSE)
class(counts) <- "data.frame"
rownames(counts) <- counts$GeneID
counts <- counts[,-1]
counts <- as.matrix(counts)
storage.mode(counts) <- "double"
counts <- t(counts)
ids <- rownames(counts)
```

Load the sample information:

```{r load-sample-info, message=FALSE}
geo <- getGEO(filename = "data/GSE152749_family.soft.gz")
samples <- data.frame(id = names(GSMList(geo)),
                      treatment = sapply(GSMList(geo),
                                         function (x) Meta(x)$title))
samples <- samples[ids,]
rownames(samples) <- NULL
samples <- transform(samples,
                     EtOH = grepl("EtOH",treatment,fixed = TRUE),
                     RA   = grepl("RA",treatment,fixed = TRUE),
                     TGFb = grepl("TGFb",treatment,fixed = TRUE))
samples$label                 <- "EtOH"
samples[samples$RA,"label"]   <- "RA"
samples[samples$TGFb,"label"] <- "TGFb"
samples[with(samples,RA & TGFb),"label"] <- "RA+TGFb"
samples <- transform(samples,
                     label = factor(label,c("EtOH","RA","TGFb","RA+TGFb")))
```

Remove the non-protein-coding genes, and the genes that are expressed
in fewer than 4 samples:

```{r filter-genes}
x <- colSums(counts > 0)
i <- which(x > 3 &
           genes$GeneType == "protein-coding" &
		   genes$Status == "active")
genes  <- genes[i,]
counts <- counts[,i]
```

(It turns out that there is some structure in the non-coding RNA genes
that is is picked up by some of the methods, so to avoid this
complication I have removed these genes from the data.)

## PCA

Let's see what happens when we apply PCA to the shifted log counts.

First compute the shifted log counts:

```{r shifted-log-counts}
a <- 1
s <- rowSums(counts)
s <- s/mean(s)
Y <- log1p(counts/(a*s))
```

Now run PCA on the shifted log counts, and plot the first 2 PCs:

```{r pca, fig.height=2, fig.width=3}
pca <- rpca(Y,k = 2,center = TRUE,scale = FALSE)
colnames(pca$x) <- c("PC1","PC2")
pdat <- data.frame(samples,pca$x)
ggplot(pdat,aes(x = PC1,y = PC2,color = label)) +
  geom_point() +
  scale_color_manual(values = c("dodgerblue","tomato","darkblue",
                                "limegreen")) + 
  theme_cowplot(font_size = 10)
```

## fastglmpca

Just for interest, let's see what happens when we compute a GLM-PCA
reduced representation of the counts.

```{r fastglmpca, fig.height=2, fig.width=3, message=FALSE}
fit_glmpca <- init_glmpca_pois(t(counts),K = 2)
fit_glmpca <- fit_glmpca_pois(t(counts),fit0 = fit_glmpca,verbose = FALSE)
colnames(fit_glmpca$V) <- c("k1","k2")
pdat <- data.frame(samples,fit_glmpca$V)
ggplot(pdat,aes(x = k1,y = k2,color = label)) +
  geom_point() +
  scale_color_manual(values = c("dodgerblue","tomato","darkblue",
                                "limegreen")) + 
  theme_cowplot(font_size = 10)
```

The top 2 PCs from GLM-PCA are essentially identical to the top 2
PCs from PCA applied to the shifted log counts:

```{r compare-pcs}
round(cor(pca$x,fit_glmpca$V),digits = 4)
```

## Topic model

Let's now try fitting a (multinomial) topic model to these data,
which represents an *additive model* of the transcriptional changes in
response to the different exposures. After a bit of trial and error,
it turns out that 3 topics are needed to get the right result:

```{r fit-topic-model}
fit0 <- fit_poisson_nmf(counts,k = 3,init.method = "random",numiter = 50,
                        verbose = "none",
                        control = list(nc = 4,extrapolate = FALSE))
fit <- fit_poisson_nmf(counts,fit0 = fit0,numiter = 50,verbose = "none",
                       control = list(nc = 4,extrapolate = TRUE))
```

The topic model identifies one topic for the transcriptional responses
to RA, one topic for the responses TGF-$\beta$, and one topic for the
"control" or "baseline" (ethanol), and the response from exposure to
RA + TGF-$\beta$ is a fairly even mixture of the RA and TGF-$\beta$
topics.

```{r structure-plot-topic-model, fig.height=1.75, fig.width=5}
topic_colors <- c("tomato","darkblue","dodgerblue")
structure_plot(fit,grouping = samples$label,topics = 1:3,
               colors = topic_colors)
```

## NMF on the shifted log counts

Now let's try fitting an NMF to the shifted log counts using flashier.
Implicitly, this model is a *multiplicative model* of the
transcriptional changes in response to the different exposures.

Next fit an NMF to the shifted log counts using flashier:

```{r flashier-nmf, cache=TRUE}
n  <- nrow(counts)
x  <- rpois(1e7,1/n)
s1 <- sd(log(x + 1))
fl_nmf <- flash(Y,ebnm_fn = ebnm_point_exponential,var_type = 2, 
                greedy_Kmax = 4,S = s1,backfit = TRUE,verbose = 0)
```

Note that I chose "greedy_Kmax" after some trial and error and found
that 4 was a good setting.

```{r structure-plot-nmf, fig.height=2.25, fig.width=5}
topic_colors <- c("olivedrab","dodgerblue","darkblue","tomato")
res <- ldf(fl_nmf,type = "i")
structure_plot(res$L,grouping = samples$label,topics = 4:1,
               colors = topic_colors) +
  labs(y = "membership")
```

Interestingly, the NMF model applied to the shifted log count
decomposes the samples in nearly the same way as the topic model, with
one key difference: it also explicitly has a *baseline factor* (factor
1).

## NMF (second attempt)

Out of curiosity, I also tried running a "vanilla" NMF method on the
shifted log counts just for comparison with EBNMF. Note that I
initialized the NMF in this way to make it more similar to the
flashier result.

```{r nnlm, warning=FALSE}
library(NNLM)
k <- 4
nmf0 <- nnmf(Y,k = 1,loss = "mse",method = "scd",
            max.iter = 10,verbose = 0,n.threads = 4)
W0 <- nmf0$W
H0 <- nmf0$H
n  <- nrow(Y)
m  <- ncol(Y)
W0 <- cbind(W0,matrix(runif(n*(k-1)),n,k-1))
H0 <- rbind(H0,matrix(runif(m*(k-1)),k-1,m))
nmf <- nnmf(Y,k,init = list(W = W0,H = H0),loss = "mse",
            method = "scd",max.iter = 100,verbose = 0,
			n.threads = 4)
```

It is actually quite interesting to see that the loadings on the
individual factors appear to be a bit "noisier" and less sparse:

```{r structure-plot-nnlm, fig.height=2.25, fig.width=5}
topic_colors <- c("olivedrab","darkblue","tomato","dodgerblue")
scale_cols <- function (A, b)
  t(t(A) * b)
W <- nmf$W
k <- ncol(W)
d <- apply(W,2,max)
W <- scale_cols(W,1/d)
structure_plot(W,grouping = samples$label,topics = c(3,2,4,1),
               colors = topic_colors) +
  labs(y = "membership")
```

## log1p NMF

Let's now compare the above NMFs—which includes the topic model since
it is also an NMF—with the new Poisson NMF we are proposing that used
the shifted log link (we call it "log1p NMF" for short). The shifted
log link has a parameter, $c$, that controls the behaviour of the link
function. Let's first try a "large" setting, $c = 10$:

```{r log1pNMF-c10, cache=TRUE}
set.seed(1)
fit_log1p_c10 <- 
  fit_poisson_log1p_nmf(counts,K = 4,cc = 10,loglik = "exact",
                        init_method = "rank1",
                        control = list(maxiter = 1000,
                                       verbose = FALSE))
```

As expected, the log1p NMF model with $c = 10$ closely recovers the
topic model (aside from the inclusion of a "baseline" factor):

```{r structure-plot-log1pNMF-c10, fig.height=2.25, fig.width=5}
topic_colors <- c("olivedrab","dodgerblue","darkblue","tomato")
L <- fit_log1p_c10$LL
d <- apply(L,2,max)
L <- scale_cols(L,1/d)
structure_plot(L,grouping = samples$label,topics = 4:1,
               colors = topic_colors) +
  labs(y = "membership")
```

Compare log1pNMF with $c = 10$ to the topic model:

```{r topic-model-vs-log1pNMF, fig.height=3.5, fig.width=3}
L1 <- fit$L
d  <- apply(L1,2,max)
L1 <- scale_cols(L1,1/d)
L2 <- fit_log1p_c10$LL
d  <- apply(L2,2,max)
L2 <- scale_cols(L2,1/d)
plot(log10(pmax(0.01,L1)),log10(pmax(0.01,L2[,4:2])),pch = 20,
     xlab = "topic model",
     ylab = "log1pNMF, c = 10")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

Let's now try a "small" setting, $c = 0.1$:

```{r log1pNMF-c01, cache=TRUE}
set.seed(1)
fit_log1p_c0.1 <- 
  fit_poisson_log1p_nmf(counts,K = 4,cc = 0.1,loglik = "exact",
                        init_method = "rank1",
                        control = list(maxiter = 1000,
                                       verbose = FALSE))
```

Interestingly, the fit at this setting is different from all the
previous results in one important respect: the "ethanol factor" does
not contribute much to the RA samples.

```{r structure-plot-log1pNMF-c01, fig.height=2.25, fig.width=5}
topic_colors <- c("olivedrab","dodgerblue","darkblue","tomato")
L <- fit_log1p_c0.1$LL
d <- apply(L,2,max)
L <- scale_cols(L,1/d)
structure_plot(L,grouping = samples$label,topics = c(4,2,3,1),
               colors = topic_colors) +
  labs(y = "membership")
```

It is also a bit weird that one of the samples is a combination of all
4 factors. This result suggests that this is quite different from the
others, i.e., an "outlier". This needs to be investigated further.

And, indeed, the "middle ground" setting of $c = 1$ produces a result
somewhere in between the two settings:

```{r log1pNMF-c1, cache=TRUE}
set.seed(1)
fit_log1p_c1 <- 
  fit_poisson_log1p_nmf(counts,K = 4,cc = 1,loglik = "exact",
                        init_method = "rank1",
                        control = list(maxiter = 1000,
                                       verbose = FALSE))
```

```{r structure-plot-log1pNMF-c1, fig.height=2.25, fig.width=5} 
topic_colors <- c("olivedrab","dodgerblue","darkblue","tomato")
L <- fit_log1p_c1$LL
d <- apply(L,2,max)
L <- scale_cols(L,1/d)
structure_plot(L,grouping = samples$label,topics = 4:1,
               colors = topic_colors) +
  labs(y = "membership")
```

Compare log1pNMF with $c = 1$ to NMF:

```{r nmf-vs-log1pNMF, fig.height=3.5, fig.width=3}
W <- nmf$W
k <- ncol(W)
d <- apply(W,2,max)
W <- scale_cols(W,1/d)
L <- fit_log1p_c1$LL
d <- apply(L,2,max)
L <- scale_cols(L,1/d)
plot(log10(pmax(0.01,W)),
     log10(pmax(0.01,L[,c(1,3,4,2)])),
	 pch = 20,xlab = "NMF",ylab = "log1pNMF, c = 1")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

Here's a plot showing the model fitting progress, specifically the
improvement in the log-likelihood at each iteration of the algorithm:

```{r plot-progress, fig.height=3.5, fig.width=3}
x1 <- diff(fit_log1p_c10$objective_trace)
x2 <- diff(fit_log1p_c1$objective_trace)
x3 <- diff(fit_log1p_c0.1$objective_trace)
plot(1:length(x1),x1,type = "l",lwd = 1.5,
     xlab = "iteration",ylab = "change in loglik",
     log = "y",col = "darkblue",ylim = c(10,1e8))
lines(1:length(x2),x2,lwd = 1.5,col = "darkorange")
lines(1:length(x3),x3,lwd = 1.5,col = "limegreen")
```

Next, I wanted to try optimizing log1p NMF models for a wide range of values of $c$ for a large number of iterations. The code to do this is below:

```{r, eval=FALSE}
K <- 4
cc_vec <- c(1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000)

fit_list <- list()

for (cc in cc_vec) {

  set.seed(1)
  fit <- fit_poisson_log1p_nmf(
    Y = counts,
    K = K,
    cc = cc,
    loglik = "exact",
    control = list(maxiter = 2500)
  )

  fit_list[[as.character(cc)]] <- fit

}

# finally, fit a topic model with a rank 1 initialization here
r1_fit <- fastTopics:::fit_pnmf_rank1(counts)

init_LL <- cbind(
  r1_fit$L,
  matrix(
    data = 1e-8,
    nrow = nrow(counts),
    ncol = K - 1
  )
)
rownames(init_LL) <- rownames(counts)

init_FF <- cbind(
  r1_fit$F,
  matrix(
    data = 1e-8,
    nrow = ncol(counts),
    ncol = K - 1
  )
)
rownames(init_FF) <- colnames(counts)

fit0 <- init_poisson_nmf(X = counts, L = init_LL, F = init_FF)

nmf_fit <- fit_poisson_nmf(
  X = counts,
  fit0 = fit0,
  numiter = 2500,
  control = list(nc = 7)
)

fit_list[[as.character(Inf)]] <- nmf_fit
```

```{r, include = FALSE}
cc_vec <- c(1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000)
load("data/experiment_results.Rdata")
fit_list <- res_list$mcf7
K <- 4
```

```{r}
hoyer_sparsity <- function(x) {

  n <- length(x)
  (1 / (sqrt(n) - 1)) * (sqrt(n) - (sum(x) / (sqrt(sum(x ^ 2)))))

}

for (cc in cc_vec) {

  fit <- fit_list[[as.character(cc)]]
  fit_list[[as.character(cc)]]$l_sparsity <- median(apply(
    fit$LL, 2, hoyer_sparsity
  ))
  fit_list[[as.character(cc)]]$f_sparsity <- median(apply(
    fit$FF, 2, hoyer_sparsity
  ))

  fit_list[[as.character(cc)]]$f_cor <- median(
    abs(cor(fit$FF, method = "spearman"))[lower.tri(diag(K))]
  )

}

fit <- fit_list[[as.character(Inf)]]

fit_list[[as.character(Inf)]]$l_sparsity <- median(apply(
  fit$L, 2, hoyer_sparsity
))
fit_list[[as.character(Inf)]]$f_sparsity <- median(apply(
  fit$F, 2, hoyer_sparsity
))


fit_list[[as.character(Inf)]]$f_cor <- median(
  abs(cor(fit$F, method = "spearman"))[lower.tri(diag(K))]
)

l_sparsity_vec <- unlist(lapply(fit_list, function(x) {x$l_sparsity}))
f_sparsity_vec <- unlist(lapply(fit_list, function(x) {x$f_sparsity}))
f_cor_vec <- unlist(lapply(fit_list, function(x) {x$f_cor}))

library(dplyr)
df_sparsity_l <- data.frame(
  cc = as.numeric(names(l_sparsity_vec)),
  sparsity = l_sparsity_vec
) %>% filter(is.finite(cc))

df_sparsity_f <- data.frame(
  cc = as.numeric(names(f_sparsity_vec)),
  sparsity = f_sparsity_vec
) %>% filter(is.finite(cc))
```

First, we examine sparsity of $L$ and $F$ across values of $c$

```{r}
hoyer_sparsity <- function(x) {

  n <- length(x)
  (1 / (sqrt(n) - 1)) * (sqrt(n) - (sum(x) / (sqrt(sum(x ^ 2)))))

}

for (cc in cc_vec) {

  fit <- fit_list[[as.character(cc)]]
  fit_list[[as.character(cc)]]$l_sparsity <- median(apply(
    fit$LL, 2, hoyer_sparsity
  ))
  fit_list[[as.character(cc)]]$f_sparsity <- median(apply(
    fit$FF, 2, hoyer_sparsity
  ))

  fit_list[[as.character(cc)]]$f_cor <- median(
    abs(cor(fit$FF, method = "spearman"))[lower.tri(diag(K))]
  )
  
  fit_list[[as.character(cc)]]$ll <- logLik(
    fit, Y = counts
  )

}

fit <- fit_list[[as.character(Inf)]]

fit_list[[as.character(Inf)]]$ll <- sum(loglik_poisson_nmf(X = counts, fit))

fit_list[[as.character(Inf)]]$l_sparsity <- median(apply(
  fit$L, 2, hoyer_sparsity
))
fit_list[[as.character(Inf)]]$f_sparsity <- median(apply(
  fit$F, 2, hoyer_sparsity
))


fit_list[[as.character(Inf)]]$f_cor <- median(
  abs(cor(fit$F, method = "spearman"))[lower.tri(diag(K))]
)

ll_vec <- unlist(lapply(fit_list, function(x) {x$ll}))
l_sparsity_vec <- unlist(lapply(fit_list, function(x) {x$l_sparsity}))
f_sparsity_vec <- unlist(lapply(fit_list, function(x) {x$f_sparsity}))
f_cor_vec <- unlist(lapply(fit_list, function(x) {x$f_cor}))

library(dplyr)
df_sparsity_l <- data.frame(
  cc = as.numeric(names(l_sparsity_vec)),
  sparsity = l_sparsity_vec
) %>% filter(is.finite(cc))

df_sparsity_f <- data.frame(
  cc = as.numeric(names(f_sparsity_vec)),
  sparsity = f_sparsity_vec
) %>% filter(is.finite(cc))

df_ll <- data.frame(
  cc = as.numeric(names(ll_vec)),
  ll = ll_vec
) %>% filter(is.finite(cc))

g1 <- ggplot(data = df_sparsity_l, aes(x = cc, y = sparsity)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1e-4, 1e-2, 1, 1e2, 1e4), transform = "log10") +
  xlab("c (log10 scale)") +
  ylab("Median Loading Sparsity") +
  geom_hline(yintercept = l_sparsity_vec["Inf"], color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.004, y=l_sparsity_vec["Inf"] + 0.05, label="ID Link", color="red"
  )

g2 <- ggplot(data = df_sparsity_f, aes(x = cc, y = sparsity)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1e-4, 1e-2, 1, 1e2, 1e4), transform = "log10") +
  xlab("c (log10 scale)") +
  ylab("Median Factor Sparsity") +
  geom_hline(yintercept = f_sparsity_vec["Inf"], color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.004, y=f_sparsity_vec["Inf"] + 0.05, label="ID Link", color="red"
  )

ggpubr::ggarrange(g1, g2, nrow = 1, ncol = 2)
```

In addition, we look at the rank correlation of $F$.

```{r}
df_cor <- data.frame(
  cc = as.numeric(names(f_cor_vec)),
  correlation = f_cor_vec
) %>% filter(is.finite(cc))

ggplot(data = df_cor, aes(x = cc, y = correlation)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1e-4, 1e-2, 1, 1e2, 1e4), transform = "log10") +
  xlab("c (log10 scale)") +
  ylab("Median Abs. Factor Correlation") +
  geom_hline(yintercept = f_cor_vec["Inf"], color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.004, y=f_cor_vec["Inf"] + 0.05, label="ID Link", color="red"
  )
```

Finally, we plot the log-likelihood per fit:

```{r}
ggplot(data = df_ll, aes(x = cc, y = ll)) +
  geom_point() +
  geom_line() +
  cowplot::theme_cowplot() +
  scale_x_continuous(breaks = c(1e-4, 1e-2, 1, 1e2, 1e4), transform = "log10") +
  xlab("c (log10 scale)") +
  ylab("loglik") +
  geom_hline(yintercept = ll_vec["Inf"], color = "red", linetype = "dashed") +
  ggplot2::annotate(
    geom="text", x=0.004, y=ll_vec["Inf"] + 10000, label="ID Link", color="red"
  )
```


[sanford2020elife]: https://doi.org/10.7554/eLife.59388

